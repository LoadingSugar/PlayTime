{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "isolated-stroke",
   "metadata": {},
   "source": [
    "## 2. 模型结构与原理\n",
    "\n",
    "### 2.1 Input 和Embedding层\n",
    "\n",
    "输入层的特征， 文章指定了稀疏离散特征居多， 这种特征我们也知道一般是先one-hot, 然后会通过embedding，处理成稠密低维的。 所以这两层还是和之前一样，假设$\\mathbf{v}_{\\mathbf{i}} \\in \\mathbb{R}^{k}$为第$i$个特征的embedding向量， 那么$\\mathcal{V}_{x}=\\left\\{x_{1} \\mathbf{v}_{1}, \\ldots, x_{n} \\mathbf{v}_{n}\\right\\}$表示的下一层的输入特征。这里带上了$x_i$是因为很多$x_i$转成了One-hot之后，出现很多为0的， 这里的$\\{x_iv_i\\}$是$x_i$不等于0的那些特征向量。  \n",
    "\n",
    "### 2.2 Bi-Interaction Pooling layer\n",
    "\n",
    "在Embedding层和神经网络之间加入了特征交叉池化层是本网络的核心创新了，正是因为这个结构，实现了FM与DNN的无缝连接， 组成了一个大的网络，且能够正常的反向传播。假设$\\mathcal{V}_{x}$是所有特征embedding的集合， 那么在特征交叉池化层的操作：\n",
    "\n",
    "$$\n",
    "f_{B I}\\left(\\mathcal{V}_{x}\\right)=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} x_{i} \\mathbf{v}_{i} \\odot x_{j} \\mathbf{v}_{j}\n",
    "$$\n",
    "\n",
    "$\\odot$表示两个向量的元素积操作，即两个向量对应维度相乘得到的元素积向量（可不是点乘呀），其中第$k$维的操作：\n",
    "$$\n",
    "\\left(v_{i} \\odot v_{j}\\right)_{k}=\\boldsymbol{v}_{i k} \\boldsymbol{v}_{j k}\n",
    "$$\n",
    "\n",
    "这便定义了在embedding空间特征的二阶交互，这个不仔细看会和感觉FM的最后一项很像，但是不一样，一定要注意这个地方不是两个隐向量的内积，而是元素积，也就是这一个交叉完了之后k个维度不求和，最后会得到一个$k$维向量，而FM那里内积的话最后得到一个数， 在进行两两Embedding元素积之后，对交叉特征向量取和， 得到该层的输出向量， 很显然， 输出是一个$k$维的向量。\n",
    "\n",
    "注意， 之前的FM到这里其实就完事了， 上面就是输出了，而这里很大的一点改进就是加入特征池化层之后， 把二阶交互的信息合并， 且上面接了一个DNN网络， 这样就能够增强FM的表达能力了， 因为FM只能到二阶， 而这里的DNN可以进行多阶且非线性，只要FM把二阶的学习好了， DNN这块学习来会更加容易， 作者在论文中也说明了这一点，且通过后面的实验证实了这个观点。\n",
    "\n",
    "如果不加DNN， NFM就退化成了FM，所以改进的关键就在于加了一个这样的层，组合了一下二阶交叉的信息，然后又给了DNN进行高阶交叉的学习，成了一种“加强版”的FM。\n",
    "\n",
    "Bi-Interaction层不需要额外的模型学习参数，更重要的是它在一个线性的时间内完成计算，和FM一致的，即时间复杂度为$O\\left(k N_{x}\\right)$，$N_x$为embedding向量的数量。参考FM，可以将上式转化为：\n",
    "$$\n",
    "f_{B I}\\left(\\mathcal{V}_{x}\\right)=\\frac{1}{2}\\left[\\left(\\sum_{i=1}^{n} x_{i} \\mathbf{v}_{i}\\right)^{2}-\\sum_{i=1}^{n}\\left(x_{i} \\mathbf{v}_{i}\\right)^{2}\\right]\n",
    "$$\n",
    "后面代码复现NFM就是用的这个公式直接计算，比较简便且清晰。\n",
    "\n",
    "### 2.3 隐藏层\n",
    "\n",
    "这一层就是全连接的神经网络， DNN在进行特征的高层非线性交互上有着天然的学习优势，公式如下：\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\mathbf{z}_{1}=&\\sigma_{1}\\left(\\mathbf{W}_{1} f_{B I} \n",
    "\\left(\\mathcal{V}_{x}\\right)+\\mathbf{b}_{1}\\right)  \\\\\n",
    "\\mathbf{z}_{2}=& \\sigma_{2}\\left(\\mathbf{W}_{2} \\mathbf{z}_{1}+\\mathbf{b}_{2}\\right) \\\\\n",
    "\\ldots \\ldots \\\\\n",
    "\\mathbf{z}_{L}=& \\sigma_{L}\\left(\\mathbf{W}_{L} \\mathbf{z}_{L-1}+\\mathbf{b}_{L}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "这里的$\\sigma_i$是第$i$层的激活函数，可不要理解成sigmoid激活函数。\n",
    "\n",
    "### 2.4 预测层\n",
    "\n",
    "这个就是最后一层的结果直接过一个隐藏层，但注意由于这里是回归问题，没有加sigmoid激活：\n",
    "$$\n",
    "f(\\mathbf{x})=\\mathbf{h}^{T} \\mathbf{z}_{L}\n",
    "$$\n",
    "\n",
    "所以， NFM模型的前向传播过程总结如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{y}_{N F M}(\\mathbf{x}) &=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i} \\\\\n",
    "&+\\mathbf{h}^{T} \\sigma_{L}\\left(\\mathbf{W}_{L}\\left(\\ldots \\sigma_{1}\\left(\\mathbf{W}_{1} f_{B I}\\left(\\mathcal{V}_{x}\\right)+\\mathbf{b}_{1}\\right) \\ldots\\right)+\\mathbf{b}_{L}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "这就是NFM模型的全貌， NFM相比较于其他模型的核心创新点是特征交叉池化层，基于它，实现了FM和DNN的无缝连接，使得DNN可以在底层就学习到包含更多信息的组合特征，这时候，就会减少DNN的很多负担，只需要很少的隐藏层就可以学习到高阶特征信息。NFM相比之前的DNN， 模型结构更浅，更简单，但是性能更好，训练和调参更容易。集合FM二阶交叉线性和DNN高阶交叉非线性的优势，非常适合处理稀疏数据的场景任务。在对NFM的真实训练过程中，也会用到像Dropout和BatchNormalization这样的技术来缓解过拟合和在过大的改变数据分布。\n",
    "\n",
    "下面通过代码看下NFM的具体实现过程， 学习一些细节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-return",
   "metadata": {},
   "source": [
    "## 3. 代码实现\n",
    "\n",
    "下面我们看下NFM的代码复现，这里主要是给大家说一下这个模型的设计逻辑，参考了deepctr的函数API的编程风格， 具体的代码以及示例大家可以去参考后面的GitHub，里面已经给出了详细的注释， 这里主要分析模型的逻辑这块。关于函数API的编程式风格，我们还给出了一份文档， 大家可以先看这个，再看后面的代码部分，会更加舒服些。下面开始：\n",
    "\n",
    "这里主要说一下NFM模型的总体运行逻辑， 这样可以让大家从宏观的层面去把握模型的设计过程， 该模型所使用的数据集是criteo数据集，具体介绍参考后面的GitHub。 数据集的特征会分为dense特征(连续)和sparse特征(离散)， 所以模型的输入层接收这两种输入。但是我们这里把输入分成了linear input和dnn input两种情况，而每种情况都有可能包含上面这两种输入。因为我们后面的模型逻辑会分这两部分走，这里有个细节要注意，就是光看上面那个NFM模型的话，是没有看到它线性特征处理的那部分的，也就是FM的前半部分公式那里图里面是没有的。但是这里我们要加上。\n",
    "$$\n",
    "\\hat{y}_{N F M}(\\mathbf{x})=w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+f(\\mathbf{x})\n",
    "$$\n",
    "所以模型的逻辑我们分成了两大部分，这里我分别给大家解释下每一块做了什么事情：\n",
    "\n",
    "1. linear part: 这部分是有关于线性计算，也就是FM的前半部分$w1x1+w2x2...wnxn+b$的计算。对于这一块的计算，我们用了一个get_linear_logits函数实现，后面再说，总之通过这个函数，我们就可以实现上面这个公式的计算过程，得到linear的输出\n",
    "2. dnn part: 这部分是后面交叉特征的那部分计算，FM的最后那部分公式f(x)。 这一块主要是针对离散的特征，首先过embedding， 然后过特征交叉池化层，这个计算我们用了get_bi_interaction_pooling_output函数实现， 得到输出之后又过了DNN网络，最后得到dnn的输出\n",
    "\n",
    "模型的最后输出结果，就是把这两个部分的输出结果加和(当然也可以加权)，再过一个sigmoid得到。所以NFM的模型定义就出来了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "undefined-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from utils import SparseFeat, DenseFeat, VarLenSparseFeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单处理特征，包括填充缺失值，数值处理，类别编码\n",
    "def data_process(data_df, dense_features, sparse_features):\n",
    "    data_df[dense_features] = data_df[dense_features].fillna(0.0)\n",
    "    for f in dense_features:\n",
    "        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)\n",
    "        \n",
    "    data_df[sparse_features] = data_df[sparse_features].fillna(\"-1\")\n",
    "    for f in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data_df[f] = lbe.fit_transform(data_df[f])\n",
    "    \n",
    "    return data_df[dense_features + sparse_features]\n",
    "\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    # 构建Input层字典，并以dense和sparse两类字典的形式返回\n",
    "    dense_input_dict, sparse_input_dict = {}, {}\n",
    "\n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            sparse_input_dict[fc.name] = Input(shape=(1, ), name=fc.name)\n",
    "        elif isinstance(fc, DenseFeat):\n",
    "            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)\n",
    "        \n",
    "    return dense_input_dict, sparse_input_dict\n",
    "\n",
    "\n",
    "def build_embedding_layers(feature_columns, input_layers_dict, is_linear):\n",
    "    # 定义一个embedding层对应的字典\n",
    "    embedding_layers_dict = dict()\n",
    "    \n",
    "    # 将特征中的sparse特征筛选出来\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    \n",
    "    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度\n",
    "    if is_linear:\n",
    "        for fc in sparse_feature_columns:\n",
    "            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, 1, name='1d_emb_' + fc.name)\n",
    "    else:\n",
    "        for fc in sparse_feature_columns:\n",
    "            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='kd_emb_' + fc.name)\n",
    "    \n",
    "    return embedding_layers_dict\n",
    "\n",
    "\n",
    "def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):\n",
    "    # 将所有的dense特征的Input层，然后经过一个全连接层得到dense特征的logits\n",
    "    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))\n",
    "    dense_logits_output = Dense(1)(concat_dense_inputs)\n",
    "    \n",
    "    # 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：\n",
    "    # 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢\n",
    "    # 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高\n",
    "    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)\n",
    "    \n",
    "    # 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应\n",
    "    sparse_1d_embed = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feat_input = sparse_input_dict[fc.name]\n",
    "        embed = Flatten()(linear_embedding_layers[fc.name](feat_input))\n",
    "        sparse_1d_embed.append(embed)\n",
    "\n",
    "    # embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接\n",
    "    # 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)\n",
    "    sparse_logits_output = Add()(sparse_1d_embed)\n",
    "\n",
    "    # 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits\n",
    "    linear_part = Add()([dense_logits_output, sparse_logits_output])\n",
    "    return linear_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defined-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiInteractionPooling(Layer):\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 优化后的公式为： 0.5 * （和的平方-平方的和）  =>> B x k\n",
    "        concated_embeds_value = inputs # B x n x k\n",
    "\n",
    "        square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keepdims=False)) # B x k\n",
    "        sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keepdims=False) # B x k\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square) # B x k\n",
    "\n",
    "        return cross_term\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, input_shape[2])\n",
    "\n",
    "\n",
    "def get_bi_interaction_pooling_output(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):\n",
    "    # 只考虑sparse的二阶交叉，将所有的embedding拼接到一起\n",
    "    # 这里在实际运行的时候，其实只会将那些非零元素对应的embedding拼接到一起\n",
    "    # 并且将非零元素对应的embedding拼接到一起本质上相当于已经乘了x, 因为x中的值是1(公式中的x)\n",
    "    sparse_kd_embed = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feat_input = sparse_input_dict[fc.name]\n",
    "        _embed = dnn_embedding_layers[fc.name](feat_input) # B x 1 x k\n",
    "        sparse_kd_embed.append(_embed)\n",
    "\n",
    "    # 将所有sparse的embedding拼接起来，得到 (n, k)的矩阵，其中n为特征数，k为embedding大小\n",
    "    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x n x k\n",
    "    \n",
    "    pooling_out = BiInteractionPooling()(concat_sparse_kd_embed)\n",
    "\n",
    "    return pooling_out\n",
    "\n",
    "\n",
    "def get_dnn_logits(pooling_out):\n",
    "    # dnn层，这里的Dropout参数，Dense中的参数都可以自己设定, 论文中还说使用了BN, 但是个人觉得BN和dropout同时使用\n",
    "    # 可能会出现一些问题，感兴趣的可以尝试一些，这里就先不加上了\n",
    "    dnn_out = Dropout(0.5)(Dense(1024, activation='relu')(pooling_out))  \n",
    "    dnn_out = Dropout(0.3)(Dense(512, activation='relu')(dnn_out))\n",
    "    dnn_out = Dropout(0.1)(Dense(256, activation='relu')(dnn_out))\n",
    "\n",
    "    dnn_logits = Dense(1)(dnn_out)\n",
    "\n",
    "    return dnn_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "familiar-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFM(linear_feature_columns, dnn_feature_columns):\n",
    "    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型\n",
    "    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding\n",
    "    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))\n",
    "\n",
    "    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式\n",
    "    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层\n",
    "    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())\n",
    "\n",
    "    # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits\n",
    "    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)\n",
    "\n",
    "    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型\n",
    "    # embedding层用户构建FM交叉部分和DNN的输入部分\n",
    "    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)\n",
    "\n",
    "    # 将输入到dnn中的sparse特征筛选出来\n",
    "    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))\n",
    "\n",
    "    pooling_output = get_bi_interaction_pooling_output(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # B x (n(n-1)/2)\n",
    "    \n",
    "    # 论文中说到在池化之后加上了BN操作\n",
    "    pooling_output = BatchNormalization()(pooling_output)\n",
    "\n",
    "    dnn_logits = get_dnn_logits(pooling_output)\n",
    "    \n",
    "    # 将linear,dnn的logits相加作为最终的logits\n",
    "    output_logits = Add()([linear_logits, dnn_logits])\n",
    "\n",
    "    # 这里的激活函数使用sigmoid\n",
    "    output_layers = Activation(\"sigmoid\")(output_logits)\n",
    "\n",
    "    model = Model(input_layers, output_layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "african-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/10\n",
      "160/160 [==============================] - 7s 46ms/sample - loss: 1.0229 - binary_crossentropy: 1.0229 - auc: 0.5929 - val_loss: 1.2334 - val_binary_crossentropy: 1.2334 - val_auc: 0.7080\n",
      "Epoch 2/10\n",
      "160/160 [==============================] - 0s 486us/sample - loss: 0.9172 - binary_crossentropy: 0.9172 - auc: 0.6102 - val_loss: 1.1540 - val_binary_crossentropy: 1.1540 - val_auc: 0.7464\n",
      "Epoch 3/10\n",
      "160/160 [==============================] - 0s 549us/sample - loss: 0.7775 - binary_crossentropy: 0.7775 - auc: 0.6335 - val_loss: 1.0293 - val_binary_crossentropy: 1.0293 - val_auc: 0.7578\n",
      "Epoch 4/10\n",
      "160/160 [==============================] - 0s 499us/sample - loss: 0.5878 - binary_crossentropy: 0.5878 - auc: 0.7229 - val_loss: 0.9171 - val_binary_crossentropy: 0.9171 - val_auc: 0.7764\n",
      "Epoch 5/10\n",
      "160/160 [==============================] - 0s 474us/sample - loss: 0.5167 - binary_crossentropy: 0.5167 - auc: 0.8306 - val_loss: 0.9303 - val_binary_crossentropy: 0.9303 - val_auc: 0.7849\n",
      "Epoch 6/10\n",
      "160/160 [==============================] - 0s 486us/sample - loss: 0.3931 - binary_crossentropy: 0.3931 - auc: 0.8778 - val_loss: 0.9889 - val_binary_crossentropy: 0.9889 - val_auc: 0.7764\n",
      "Epoch 7/10\n",
      "160/160 [==============================] - 0s 530us/sample - loss: 0.3133 - binary_crossentropy: 0.3133 - auc: 0.9085 - val_loss: 1.0200 - val_binary_crossentropy: 1.0200 - val_auc: 0.7835\n",
      "Epoch 8/10\n",
      "160/160 [==============================] - 0s 511us/sample - loss: 0.2456 - binary_crossentropy: 0.2456 - auc: 0.9359 - val_loss: 1.0539 - val_binary_crossentropy: 1.0539 - val_auc: 0.7536\n",
      "Epoch 9/10\n",
      "160/160 [==============================] - 0s 517us/sample - loss: 0.1979 - binary_crossentropy: 0.1979 - auc: 0.9545 - val_loss: 1.0903 - val_binary_crossentropy: 1.0903 - val_auc: 0.7621\n",
      "Epoch 10/10\n",
      "160/160 [==============================] - 0s 502us/sample - loss: 0.1482 - binary_crossentropy: 0.1482 - auc: 0.9693 - val_loss: 1.1366 - val_binary_crossentropy: 1.1366 - val_auc: 0.7464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26582a13908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./data/criteo_sample.txt')\n",
    "\n",
    "# 划分dense和sparse特征\n",
    "columns = data.columns.values\n",
    "dense_features = [feat for feat in columns if 'I' in feat]\n",
    "sparse_features = [feat for feat in columns if 'C' in feat]\n",
    "\n",
    "# 简单的数据预处理\n",
    "train_data = data_process(data, dense_features, sparse_features)\n",
    "train_data['label'] = data['label']\n",
    "\n",
    "# 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）\n",
    "linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n",
    "                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                        for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n",
    "                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                        for feat in dense_features]\n",
    "\n",
    "# 构建NFM模型\n",
    "history = NFM(linear_feature_columns, dnn_feature_columns)\n",
    "#history.summary()\n",
    "history.compile(optimizer=\"adam\", \n",
    "            loss=\"binary_crossentropy\", \n",
    "            metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# 将输入数据转化成字典的形式输入\n",
    "train_model_input = {name: data[name] for name in dense_features + sparse_features}\n",
    "# 模型训练\n",
    "history.fit(train_model_input, train_data['label'].values,\n",
    "        batch_size=64, epochs=10, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tropical-lingerie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[1],\n",
       "       [2]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([[1,2]])\n",
    "tf.reshape(a,[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-guard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 2]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-liabilities",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1.0-gpu",
   "language": "python",
   "name": "tf2.1.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
