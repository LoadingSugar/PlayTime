{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attempted-camcorder",
   "metadata": {},
   "source": [
    "### 2.1 FM\n",
    "\n",
    "详细内容参考FM模型部分的内容，下图是FM的一个结构图，从图中大致可以看出FM Layer是由一阶特征和二阶特征Concatenate到一起在经过一个Sigmoid得到logits（结合FM的公式一起看），所以在实现的时候需要单独考虑linear部分和FM交叉特征部分。\n",
    "$$\n",
    "\\hat{y}_{FM}(x) = w_0+\\sum_{i=1}^N w_ix_i + \\sum_{i=1}^N \\sum_{j=i+1}^N v_i^T v_j x_ix_j\n",
    "$$\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181340313.png\" alt=\"image-20210225181340313\" style=\"zoom: 67%;\" />\n",
    "\n",
    "### 2.2 Deep\n",
    "\n",
    "Deep架构图\n",
    "\n",
    "<img src=\"http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181010107.png\" alt=\"image-20210225181010107\" style=\"zoom:50%;\" />\n",
    "\n",
    "Deep Module是为了学习高阶的特征组合，在上图中使用用全连接的方式将Dense Embedding输入到Hidden Layer，这里面Dense Embeddings就是为了解决DNN中的参数爆炸问题，这也是推荐模型中常用的处理方法。\n",
    "\n",
    "Embedding层的输出是将所有id类特征对应的embedding向量concat到到一起输入到DNN中。其中$v_i$表示第i个field的embedding，m是field的数量。\n",
    "$$\n",
    "z_1=[v_1, v_2, ..., v_m]\n",
    "$$\n",
    "上一层的输出作为下一层的输入，我们得到：\n",
    "$$\n",
    "z_L=\\sigma(W_{L-1} z_{L-1}+b_{L-1})\n",
    "$$\n",
    "其中$\\sigma$表示激活函数，$z, W, b $分别表示该层的输入、权重和偏置。\n",
    "\n",
    "最后进入DNN部分输出使用sigmod激活函数进行激活：\n",
    "$$\n",
    "y_{DNN}=\\sigma(W^{L}a^L+b^L)\n",
    "$$\n",
    "\n",
    "\n",
    "## 3. 代码实现\n",
    "\n",
    "DeepFM在模型的结构图中显示，模型大致由两部分组成，一部分是FM，还有一部分就是DNN, 而FM又由一阶特征部分与二阶特征交叉部分组成，所以可以将整个模型拆成三部分，分别是一阶特征处理linear部分，二阶特征交叉FM以及DNN的高阶特征交叉。在下面的代码中也能够清晰的看到这个结构。此外每一部分可能由是由不同的特征组成，所以在构建模型的时候需要分别对这三部分输入的特征进行选择。\n",
    "\n",
    "- linear_logits:  这部分是有关于线性计算，也就是FM的前半部分$w1x1+w2x2...wnxn+b$的计算。对于这一块的计算，我们用了一个get_linear_logits函数实现，后面再说，总之通过这个函数，我们就可以实现上面这个公式的计算过程，得到linear的输出， 这部分特征由数值特征和类别特征的onehot编码组成的一维向量组成，实际应用中根据自己的业务放置不同的一阶特征(这里的dense特征并不是必须的，有可能会将数值特征进行分桶，然后在当做类别特征来处理)\n",
    "\n",
    "- fm_logits:  这一块主要是针对离散的特征，首先过embedding，然后使用FM特征交叉的方式，两两特征进行交叉，得到新的特征向量，最后计算交叉特征的logits\n",
    "\n",
    "- dnn_logits:   这一块主要是针对离散的特征，首先过embedding，然后将得到的embedding拼接成一个向量(具体的可以看代码，也可以看一下下面的模型结构图)，通过dnn学习类别特征之间的隐式特征交叉并输出logits值 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unknown-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import  MinMaxScaler, LabelEncoder\n",
    "\n",
    "from utils import SparseFeat, DenseFeat, VarLenSparseFeat\n",
    "from tensorflow.keras.optimizers import SGD,Nadam,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "competent-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单处理特征，包括填充缺失值，数值处理，类别编码\n",
    "def data_process(data_df, dense_features, sparse_features):\n",
    "    data_df[dense_features] = data_df[dense_features].fillna(0.0)\n",
    "    for f in dense_features:\n",
    "        data_df[f] = data_df[f].apply(lambda x: np.log(x+1) if x > -1 else -1)\n",
    "        \n",
    "    data_df[sparse_features] = data_df[sparse_features].fillna(\"-1\")\n",
    "    for f in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data_df[f] = lbe.fit_transform(data_df[f])\n",
    "    \n",
    "    return data_df[dense_features + sparse_features]\n",
    "\n",
    "\n",
    "def build_input_layers(feature_columns):\n",
    "    # 构建Input层字典，并以dense和sparse两类字典的形式返回\n",
    "    dense_input_dict, sparse_input_dict = {}, {}\n",
    "\n",
    "    for fc in feature_columns:\n",
    "        if isinstance(fc, SparseFeat):\n",
    "            sparse_input_dict[fc.name] = Input(shape=(1, ), name=fc.name)\n",
    "        elif isinstance(fc, DenseFeat):\n",
    "            dense_input_dict[fc.name] = Input(shape=(fc.dimension, ), name=fc.name)\n",
    "        \n",
    "    return dense_input_dict, sparse_input_dict\n",
    "\n",
    "\n",
    "def build_embedding_layers(feature_columns, input_layers_dict, is_linear):\n",
    "    # 定义一个embedding层对应的字典\n",
    "    embedding_layers_dict = dict()\n",
    "    \n",
    "    # 将特征中的sparse特征筛选出来\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []\n",
    "    \n",
    "    # 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度\n",
    "    if is_linear:\n",
    "        for fc in sparse_feature_columns:\n",
    "            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, 1, name='1d_emb_' + fc.name)\n",
    "    else:\n",
    "        for fc in sparse_feature_columns:\n",
    "            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size, fc.embedding_dim, name='kd_emb_' + fc.name)\n",
    "    \n",
    "    return embedding_layers_dict\n",
    "\n",
    "\n",
    "def get_linear_logits(dense_input_dict, sparse_input_dict, sparse_feature_columns):\n",
    "    # 将所有的dense特征的Input层，然后经过一个全连接层得到dense特征的logits\n",
    "    concat_dense_inputs = Concatenate(axis=1)(list(dense_input_dict.values()))\n",
    "    dense_logits_output = Dense(1)(concat_dense_inputs)\n",
    "    \n",
    "    # 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：\n",
    "    # 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢\n",
    "    # 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高\n",
    "    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=True)\n",
    "    \n",
    "    # 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应\n",
    "    sparse_1d_embed = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feat_input = sparse_input_dict[fc.name]\n",
    "        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) # B x 1\n",
    "        sparse_1d_embed.append(embed)\n",
    "\n",
    "    # embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接\n",
    "    # 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)\n",
    "    sparse_logits_output = Add()(sparse_1d_embed)\n",
    "\n",
    "    # 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits\n",
    "    linear_logits = Add()([dense_logits_output, sparse_logits_output])\n",
    "    return linear_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "undefined-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM_Layer(Layer):\n",
    "    def __init__(self):\n",
    "        super(FM_Layer, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 优化后的公式为： 0.5 * 求和（和的平方-平方的和）  =>> B x 1\n",
    "        concated_embeds_value = inputs # B x n x k\n",
    "\n",
    "        square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis=1, keepdims=True)) # B x 1 x k\n",
    "        sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis=1, keepdims=True) # B x1 xk\n",
    "        cross_term = square_of_sum - sum_of_square # B x 1 x k\n",
    "        cross_term = 0.5 * tf.reduce_sum(cross_term, axis=2, keepdims=False) # B x 1\n",
    "\n",
    "        return cross_term\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, 1)\n",
    "\n",
    "\n",
    "def get_fm_logits(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):\n",
    "    # 将特征中的sparse特征筛选出来\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), sparse_feature_columns))\n",
    "\n",
    "    # 只考虑sparse的二阶交叉，将所有的embedding拼接到一起进行FM计算\n",
    "    # 因为类别型数据输入的只有0和1所以不需要考虑将隐向量与x相乘，直接对隐向量进行操作即可\n",
    "    sparse_kd_embed = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feat_input = sparse_input_dict[fc.name]\n",
    "        _embed = dnn_embedding_layers[fc.name](feat_input) # B x 1 x k\n",
    "        sparse_kd_embed.append(_embed)\n",
    "\n",
    "    # 将所有sparse的embedding拼接起来，得到 (n, k)的矩阵，其中n为特征数，k为embedding大小\n",
    "    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x n x k\n",
    "    fm_cross_out = FM_Layer()(concat_sparse_kd_embed)\n",
    "\n",
    "    return fm_cross_out\n",
    "\n",
    "\n",
    "def get_dnn_logits(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers):\n",
    "    # 将特征中的sparse特征筛选出来\n",
    "    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), sparse_feature_columns))\n",
    "\n",
    "    # 将所有非零的sparse特征对应的embedding拼接到一起\n",
    "    sparse_kd_embed = []\n",
    "    for fc in sparse_feature_columns:\n",
    "        feat_input = sparse_input_dict[fc.name]\n",
    "        _embed = dnn_embedding_layers[fc.name](feat_input) # B x 1 x k\n",
    "        _embed = Flatten()(_embed) # B x k\n",
    "        sparse_kd_embed.append(_embed)\n",
    "\n",
    "    concat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed) # B x nk   \n",
    "\n",
    "    # dnn层，这里的Dropout参数，Dense中的参数都可以自己设定，以及Dense的层数都可以自行设定\n",
    "    mlp_out = Dropout(0.5)(Dense(256, activation='relu')(concat_sparse_kd_embed))  \n",
    "    mlp_out = Dropout(0.3)(Dense(256, activation='relu')(mlp_out))\n",
    "    mlp_out = Dropout(0.1)(Dense(256, activation='relu')(mlp_out))\n",
    "\n",
    "    dnn_out = Dense(1)(mlp_out)\n",
    "\n",
    "    return dnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "understood-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepFM(linear_feature_columns, dnn_feature_columns):\n",
    "    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型\n",
    "    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding\n",
    "    linear_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), linear_feature_columns))\n",
    "\n",
    "    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式\n",
    "    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层\n",
    "    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())\n",
    "\n",
    "    # linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits\n",
    "    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)\n",
    "\n",
    "    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型\n",
    "    # embedding层用户构建FM交叉部分和DNN的输入部分\n",
    "    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)\n",
    "\n",
    "    # 将输入到dnn中的所有sparse特征筛选出来\n",
    "    dnn_sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns))\n",
    "\n",
    "    fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) # 只考虑二阶项\n",
    "\n",
    "    # 将所有的Embedding都拼起来，一起输入到dnn中\n",
    "    dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)\n",
    "    \n",
    "    # 将linear,FM,dnn的logits相加作为最终的logits\n",
    "    output_logits = Add()([linear_logits, fm_logits, dnn_logits])\n",
    "\n",
    "    # 这里的激活函数使用sigmoid\n",
    "    output_layers = Activation(\"sigmoid\")(output_logits)\n",
    "\n",
    "    model = Model(input_layers, output_layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "reflected-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/25\n",
      "160/160 [==============================] - 5s 31ms/sample - loss: 0.7146 - binary_crossentropy: 0.7146 - auc: 0.5829 - val_loss: 0.8696 - val_binary_crossentropy: 0.8696 - val_auc: 0.5214\n",
      "Epoch 2/25\n",
      "160/160 [==============================] - 0s 745us/sample - loss: 0.6085 - binary_crossentropy: 0.6085 - auc: 0.6018 - val_loss: 0.8725 - val_binary_crossentropy: 0.8725 - val_auc: 0.5370\n",
      "Epoch 3/25\n",
      "160/160 [==============================] - 0s 760us/sample - loss: 0.5696 - binary_crossentropy: 0.5696 - auc: 0.5997 - val_loss: 0.7606 - val_binary_crossentropy: 0.7606 - val_auc: 0.5199\n",
      "Epoch 4/25\n",
      "160/160 [==============================] - 0s 772us/sample - loss: 0.5428 - binary_crossentropy: 0.5428 - auc: 0.6206 - val_loss: 0.7349 - val_binary_crossentropy: 0.7349 - val_auc: 0.5242\n",
      "Epoch 5/25\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 0.5068 - binary_crossentropy: 0.5068 - auc: 0.6625 - val_loss: 0.7489 - val_binary_crossentropy: 0.7489 - val_auc: 0.5427\n",
      "Epoch 6/25\n",
      "160/160 [==============================] - 0s 739us/sample - loss: 0.5020 - binary_crossentropy: 0.5020 - auc: 0.6817 - val_loss: 0.7162 - val_binary_crossentropy: 0.7162 - val_auc: 0.5413\n",
      "Epoch 7/25\n",
      "160/160 [==============================] - 0s 733us/sample - loss: 0.4792 - binary_crossentropy: 0.4792 - auc: 0.7129 - val_loss: 0.6901 - val_binary_crossentropy: 0.6901 - val_auc: 0.5399\n",
      "Epoch 8/25\n",
      "160/160 [==============================] - 0s 735us/sample - loss: 0.4761 - binary_crossentropy: 0.4761 - auc: 0.7292 - val_loss: 0.6878 - val_binary_crossentropy: 0.6878 - val_auc: 0.5541\n",
      "Epoch 9/25\n",
      "160/160 [==============================] - 0s 732us/sample - loss: 0.4744 - binary_crossentropy: 0.4744 - auc: 0.7343 - val_loss: 0.6941 - val_binary_crossentropy: 0.6941 - val_auc: 0.5755\n",
      "Epoch 10/25\n",
      "160/160 [==============================] - 0s 751us/sample - loss: 0.4701 - binary_crossentropy: 0.4701 - auc: 0.7434 - val_loss: 0.6640 - val_binary_crossentropy: 0.6640 - val_auc: 0.5798\n",
      "Epoch 11/25\n",
      "160/160 [==============================] - 0s 789us/sample - loss: 0.4535 - binary_crossentropy: 0.4535 - auc: 0.7754 - val_loss: 0.6842 - val_binary_crossentropy: 0.6842 - val_auc: 0.5997\n",
      "Epoch 12/25\n",
      "160/160 [==============================] - 0s 810us/sample - loss: 0.4404 - binary_crossentropy: 0.4404 - auc: 0.7989 - val_loss: 0.6615 - val_binary_crossentropy: 0.6615 - val_auc: 0.6083\n",
      "Epoch 13/25\n",
      "160/160 [==============================] - 0s 851us/sample - loss: 0.4424 - binary_crossentropy: 0.4424 - auc: 0.8056 - val_loss: 0.6536 - val_binary_crossentropy: 0.6536 - val_auc: 0.6054\n",
      "Epoch 14/25\n",
      "160/160 [==============================] - 0s 742us/sample - loss: 0.4270 - binary_crossentropy: 0.4270 - auc: 0.8180 - val_loss: 0.6649 - val_binary_crossentropy: 0.6649 - val_auc: 0.6254\n",
      "Epoch 15/25\n",
      "160/160 [==============================] - 0s 736us/sample - loss: 0.4226 - binary_crossentropy: 0.4226 - auc: 0.8264 - val_loss: 0.6479 - val_binary_crossentropy: 0.6479 - val_auc: 0.6311\n",
      "Epoch 16/25\n",
      "160/160 [==============================] - 0s 731us/sample - loss: 0.4136 - binary_crossentropy: 0.4136 - auc: 0.8421 - val_loss: 0.6406 - val_binary_crossentropy: 0.6406 - val_auc: 0.6239\n",
      "Epoch 17/25\n",
      "160/160 [==============================] - 0s 757us/sample - loss: 0.4123 - binary_crossentropy: 0.4123 - auc: 0.8497 - val_loss: 0.6395 - val_binary_crossentropy: 0.6395 - val_auc: 0.6453\n",
      "Epoch 18/25\n",
      "160/160 [==============================] - 0s 745us/sample - loss: 0.4092 - binary_crossentropy: 0.4092 - auc: 0.8426 - val_loss: 0.6470 - val_binary_crossentropy: 0.6470 - val_auc: 0.6481\n",
      "Epoch 19/25\n",
      "160/160 [==============================] - 0s 717us/sample - loss: 0.3961 - binary_crossentropy: 0.3961 - auc: 0.8686 - val_loss: 0.6319 - val_binary_crossentropy: 0.6319 - val_auc: 0.6467\n",
      "Epoch 20/25\n",
      "160/160 [==============================] - 0s 728us/sample - loss: 0.3981 - binary_crossentropy: 0.3981 - auc: 0.8769 - val_loss: 0.6286 - val_binary_crossentropy: 0.6286 - val_auc: 0.6481\n",
      "Epoch 21/25\n",
      "160/160 [==============================] - 0s 770us/sample - loss: 0.3905 - binary_crossentropy: 0.3905 - auc: 0.8737 - val_loss: 0.6414 - val_binary_crossentropy: 0.6414 - val_auc: 0.6610\n",
      "Epoch 22/25\n",
      "160/160 [==============================] - 0s 796us/sample - loss: 0.3858 - binary_crossentropy: 0.3858 - auc: 0.8815 - val_loss: 0.6273 - val_binary_crossentropy: 0.6273 - val_auc: 0.6496\n",
      "Epoch 23/25\n",
      "160/160 [==============================] - 0s 779us/sample - loss: 0.3794 - binary_crossentropy: 0.3794 - auc: 0.8887 - val_loss: 0.6357 - val_binary_crossentropy: 0.6357 - val_auc: 0.6624\n",
      "Epoch 24/25\n",
      "160/160 [==============================] - 0s 775us/sample - loss: 0.3714 - binary_crossentropy: 0.3714 - auc: 0.8973 - val_loss: 0.6263 - val_binary_crossentropy: 0.6263 - val_auc: 0.6595\n",
      "Epoch 25/25\n",
      "160/160 [==============================] - 0s 760us/sample - loss: 0.3634 - binary_crossentropy: 0.3634 - auc: 0.9164 - val_loss: 0.6208 - val_binary_crossentropy: 0.6208 - val_auc: 0.6581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b4ae69b388>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./data/criteo_sample.txt')\n",
    "\n",
    "# 划分dense和sparse特征\n",
    "columns = data.columns.values\n",
    "dense_features = [feat for feat in columns if 'I' in feat]\n",
    "sparse_features = [feat for feat in columns if 'C' in feat]\n",
    "\n",
    "# 简单的数据预处理\n",
    "train_data = data_process(data, dense_features, sparse_features)\n",
    "train_data['label'] = data['label']\n",
    "\n",
    "# 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）\n",
    "linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n",
    "                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                        for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=4)\n",
    "                        for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                        for feat in dense_features]\n",
    "\n",
    "# 构建DeepFM模型\n",
    "history = DeepFM(linear_feature_columns, dnn_feature_columns)\n",
    "#history.summary()\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / 15\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=0.01, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "adam = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "nadam = Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "history.compile(optimizer=sgd, \n",
    "            loss=\"binary_crossentropy\", \n",
    "            metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# 将输入数据转化成字典的形式输入\n",
    "train_model_input = {name: data[name] for name in dense_features + sparse_features}\n",
    "# 模型训练\n",
    "history.fit(train_model_input, train_data['label'].values,\n",
    "        batch_size=32, epochs=25, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-latino",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.1.0-gpu",
   "language": "python",
   "name": "tf2.1.0-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
